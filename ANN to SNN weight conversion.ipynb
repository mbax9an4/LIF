{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports of all libraries needed for the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "mnist_data = input_data.read_data_sets('MNIST_data',  one_hot=True)\n",
    "\n",
    "#get datasets from mnist\n",
    "training_data, training_labels = mnist_data.train.images, mnist_data.train.labels\n",
    "testing_data, testing_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "\n",
    "#define work variables that will pass the data to the network\n",
    "data_pl = tf.placeholder(tf.float32, shape=(None, mnist.IMAGE_PIXELS))\n",
    "labels_pl = tf.placeholder(tf.float32, shape=(None,10))\n",
    "\n",
    "# data.train.labels.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import and load dataset. <br/>\n",
    "The MNIST data is split into three parts: 55,000 data points of training data __(mnist.train)__, 10,000 points of test data __(mnist.test)__, and 5,000 points of validation data __(mnist.validation)__. Method that will read and return placeholders with the data and labels from the mnist dataset, stored into tensorflow variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define input parameters required for this simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_tau = tf.constant(10, dtype=tf.float32)\n",
    "rc_tau = tf.constant(5, dtype=tf.float32)\n",
    "voltage_threshold = tf.constant(3, dtype=tf.float32)\n",
    "gamma = 0.2\n",
    "one = tf.constant(1,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define all work variables needed, such as weights for the artificial neural network. Since there is no way to convert the bias variables of an artificial network into parameters of a spiking network, we will omit using them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the environment is defined we can specify the neuron model to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel(input_signal):\n",
    "    data_norm = tf.add(one,tf.exp(tf.truediv(input_signal,gamma)))\n",
    "    smoothed_data = tf.scalar_mul(gamma, tf.log(data_norm))\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lif_neuron(prev_layer):\n",
    "    kernel_res = kernel(tf.sub(prev_layer,voltage_threshold))\n",
    "    rescaled_kernel = tf.log(tf.add(one,tf.truediv(voltage_threshold, kernel_res)))\n",
    "    neuron_act = tf.truediv(one,(tf.add(reference_tau, tf.scalar_mul(rc_tau, rescaled_kernel)))) \n",
    "    return neuron_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined the dynamics of our neuron model, this will replace the ReLU or softmax activation functions typically used for artificial neurons.<br/>\n",
    "The model has been derived from the Leaky integrate and fire spiking neuron model steady state firing rate, to which some smoothing has been applied in order to make it suitable for gradient descent learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_hidden = init_weights([mnist.IMAGE_PIXELS, 625])\n",
    "weights_output = init_weights([625, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should define all the weight variable for the network, depending on how many layers we want, the weight tensors should probably be linked with their specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(data, weights_h, weights_o):\n",
    "    hidd_layer = lif_neuron(tf.matmul(data, weights_h))\n",
    "    out_pred = lif_neuron(tf.matmul(hidd_layer, weights_o))\n",
    "    return out_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns the output layer of the network, so that we can then compute the cost of the misclassifications over the training batch. We use the lif_neuron model to determine the behaviour of the individual neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neuron_pred = model(data_pl, weights_hidden, weights_output)\n",
    "# tf.Print(neuron_pred,[neuron_pred], 'the prediction is: ')\n",
    "error = tf.reduce_mean(tf.square(tf.sub(tf.nn.softmax(neuron_pred), labels_pl)))\n",
    "train_proc = tf.train.GradientDescentOptimizer(0.2).minimize(error)\n",
    "\n",
    "predict_proc = tf.argmax(neuron_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a session parameter,we can also use an interactive session parameter that would allows us to mix operations that build the computational graph with ones that run the graph, otherwise all the variables and graph parameters would have to be defined before we can run the graph, like in this case. An interactive session can be defined before this point, so it would allow us to still create more variables before it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.098000000000000004)\n",
      "(1, 0.098000000000000004)\n",
      "(2, 0.098000000000000004)\n",
      "(3, 0.098000000000000004)\n",
      "(4, 0.098000000000000004)\n",
      "(5, 0.098000000000000004)\n",
      "(6, 0.098000000000000004)\n",
      "(7, 0.098000000000000004)\n",
      "(8, 0.098000000000000004)\n",
      "(9, 0.098000000000000004)\n",
      "(10, 0.098000000000000004)\n",
      "(11, 0.098000000000000004)\n",
      "(12, 0.098000000000000004)\n",
      "(13, 0.098000000000000004)\n",
      "(14, 0.098000000000000004)\n",
      "(15, 0.098000000000000004)\n",
      "(16, 0.098000000000000004)\n",
      "(17, 0.098000000000000004)\n",
      "(18, 0.098000000000000004)\n",
      "(19, 0.098000000000000004)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for i in range(20):\n",
    "        for start, end in zip(range(0, len(training_data), 128), range(128, len(training_data+1), 128)):\n",
    "            sess.run(train_proc, feed_dict={data_pl:training_data[start:end], labels_pl:training_labels[start:end]})\n",
    "#             print(sess.run(predict_proc, feed_dict={data_pl:training_data[start:end], labels_pl:training_labels[start:end]}))\n",
    "            #tf.Print(labels_pl,[labels_pl], 'the true label is: ')\n",
    "        print(i, np.mean(np.argmax(testing_labels, axis=1) == \n",
    "                         sess.run(predict_proc, feed_dict={data_pl:testing_data, labels_pl:testing_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
